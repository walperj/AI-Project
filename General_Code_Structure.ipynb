{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7T+P6TNLxZxYVEyK0Trxl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3lgsONu61Ent"},"outputs":[],"source":["!pip install --upgrade pip\n","!pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio"]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"],"metadata":{"id":"SF8DrBSf2O7y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset, DatasetDict\n","\n","common_voice = DatasetDict()\n","\n","common_voice[\"train\"] = load_dataset(\"DTU54DL/common-accent\", split=\"train\", use_auth_token=True)\n","common_voice[\"test\"] = load_dataset(\"DTU54DL/common-accent\", split=\"test\", use_auth_token=True)\n","\n","print(common_voice)"],"metadata":{"id":"GoJ_9Y6S2Spj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["common_voice = common_voice.remove_columns([\"accent\"])\n","\n","print(common_voice)"],"metadata":{"id":"6JFDUuoh2XwG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import WhisperFeatureExtractor\n","\n","feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"],"metadata":{"id":"dIha1Nz22Z6w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import WhisperTokenizer\n","\n","tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"english\", task=\"transcribe\")"],"metadata":{"id":"ILvYQAw-2aEQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import WhisperProcessor\n","\n","processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"english\", task=\"transcribe\")"],"metadata":{"id":"fXRIpX8v2f3o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import Audio\n","\n","common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"],"metadata":{"id":"yl3A4Jbi2heF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_dataset(batch):\n","    # load and resample audio data from 48 to 16kHz\n","    audio = batch[\"audio\"]\n","\n","    # compute log-Mel input features from input audio array\n","    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n","\n","    # encode target text to label ids\n","    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n","    return batch"],"metadata":{"id":"erHs5wbD2kdx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)"],"metadata":{"id":"8QksjfG62nzC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import WhisperForConditionalGeneration\n","\n","model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"],"metadata":{"id":"-kjBFgI72qCK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.generation_config.language = \"english\"\n","model.generation_config.task = \"transcribe\"\n","\n","model.generation_config.forced_decoder_ids = None"],"metadata":{"id":"29SiA6Le2seq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","from dataclasses import dataclass\n","from typing import Any, Dict, List, Union\n","\n","@dataclass\n","class DataCollatorSpeechSeq2SeqWithPadding:\n","    processor: Any\n","    decoder_start_token_id: int\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # split inputs and labels since they have to be of different lengths and need different padding methods\n","        # first treat the audio inputs by simply returning torch tensors\n","        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n","        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n","\n","        # get the tokenized label sequences\n","        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n","        # pad the labels to max length\n","        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n","\n","        # replace padding with -100 to ignore loss correctly\n","        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n","\n","        # if bos token is appended in previous tokenization step,\n","        # cut bos token here as it's append later anyways\n","        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n","            labels = labels[:, 1:]\n","\n","        batch[\"labels\"] = labels\n","\n","        return batch"],"metadata":{"id":"5-sgiRYh2tLc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n","    processor=processor,\n","    decoder_start_token_id=model.config.decoder_start_token_id,\n",")"],"metadata":{"id":"Nxu1_m5b2zyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import evaluate\n","\n","metric = evaluate.load(\"wer\")"],"metadata":{"id":"v8DzwPX-24AK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_metrics(pred):\n","    pred_ids = pred.predictions\n","    label_ids = pred.label_ids\n","\n","    # replace -100 with the pad_token_id\n","    label_ids[label_ids == -100] = tokenizer.pad_token_id\n","\n","    # we do not want to group tokens when computing the metrics\n","    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n","    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n","\n","    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n","\n","    return {\"wer\": wer}"],"metadata":{"id":"c1nGbwv924db"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Seq2SeqTrainingArguments\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"./whisper-small-english\",  # change to a repo name of your choice\n","    per_device_train_batch_size=16,\n","    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n","    learning_rate=1e-5,\n","    warmup_steps=500,\n","    max_steps=4000,\n","    gradient_checkpointing=True,\n","    fp16=False,  # Disable mixed precision training\n","    evaluation_strategy=\"steps\",\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    generation_max_length=225,\n","    save_steps=1000,\n","    eval_steps=1000,\n","    logging_steps=25,\n","    report_to=[\"tensorboard\"],\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"wer\",\n","    greater_is_better=False,\n","    push_to_hub=False,\n",")"],"metadata":{"id":"omHolzvK28-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Seq2SeqTrainer\n","\n","trainer = Seq2SeqTrainer(\n","    args=training_args,\n","    model=model,\n","    train_dataset=common_voice[\"train\"],\n","    eval_dataset=common_voice[\"test\"],\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    tokenizer=processor.feature_extractor,\n",")"],"metadata":{"id":"HSOrRaLx3CRT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processor.save_pretrained(training_args.output_dir)"],"metadata":{"id":"-lqRd8G73C3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"NlCSVnI33EsI"},"execution_count":null,"outputs":[]}]}